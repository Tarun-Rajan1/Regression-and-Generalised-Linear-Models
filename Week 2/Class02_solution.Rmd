---
title: "ST300 Week 2"
output:
  html_document:
    df_print: paged
---

In this workshop:

- We work on the excercises 9, 10, 11, and 12 in lecture 2. 

## Residuals in Linear Models

Write a code in R that generates a sample of size 100 from \( Y = X\beta + \varepsilon \), where \( \varepsilon \sim N(0, 1) \). Calculate the residuals in two scenarios:

1. When \( X \) does not contain a column of all ones (i.e., no intercept term).
2. When \( X \) contains a column of all ones (i.e., an intercept term is included).

```{r}
set.seed(1)
# Sample size
n <- 100

# Generate the design matrix X (without a column of ones)
#whose values come from a standard normal distribution
#There are two predictors.
X_no_intercept <- matrix(rnorm(n * 2), nrow = n, ncol = 2)  # Two predictors

# Generate beta coefficients
beta <- c(2, -1)

# Generate errors from N(0, 1)
epsilon <- rnorm(n, mean = 0, sd = 1)

# Generate Y
Y <- X_no_intercept %*% beta + epsilon

# Residuals when X does not contain a column of ones
fit_no_intercept <- lm(Y ~ X_no_intercept - 1)  # -1 removes the intercept
residuals_no_intercept <- resid(fit_no_intercept)
summary(fit_no_intercept)

# Add a column of ones to X (with intercept term)
X_with_intercept <- cbind(1, X_no_intercept)

# Residuals when X contains a column of ones
fit_with_intercept <- lm(Y ~ X_with_intercept - 1)      # -1 keeps only X_with_intercept
residuals_with_intercept <- resid(fit_with_intercept)
summary(fit_with_intercept)

# Compare residuals

sum(residuals_no_intercept)
sum(residuals_with_intercept)
```

**Exercise 9**. Show that when \( X \) contains a column of all 1â€™s, i.e. we allow for an intercept in the model, the mean value of residuals is equal to zero.

**Exercise 10**. For \( Y \in \mathbb{R}^n \) and \( X \in \mathbb{R}^{n \times p} \), consider the regression \( Y = X\beta \), and let \( \hat{\beta} \) be the OLS estimate. Show that for any \( b \in \mathbb{R}^p \) we have 

\[
    \|Y - Xb\|^2 \geq \|Y - X\hat{\beta}\|^2,
\]

with equality holding if and only if \( b = \hat{\beta} \).


Consider a regression problem where \( X \in \mathbb{R}^{n \times p} \) is a matrix of predictors, and \( \Gamma \in \mathbb{R}^{p \times p} \) is a non-degenerate (invertible) transformation matrix. Let \( \tilde{X} = X\Gamma \). From the OLS fit of \( Y \) on \( X \), we obtain the coefficient \( \hat{\beta} \), fitted values \( \hat{Y} \), and residuals \( \hat{\varepsilon} \). Similarly, from the OLS fit of \( Y \) on \( \tilde{X} \), we obtain the coefficient \( \tilde{\beta} \), fitted values \( \tilde{Y} \), and residuals \( \tilde{\varepsilon} \).

Your task is to implement the following steps in R and verify the results.

## Exploring the Impact of Linear Transformation on OLS Estimation
### Step 1: Generate Data

- Set \( n = 100 \) (number of observations) and \( p = 3 \) (number of predictors).
- Simulate a matrix \( X \) with dimensions \( n \times p \), where each entry is drawn from a standard normal distribution.
- Simulate the true coefficient vector \( \beta \in \mathbb{R}^p \), for example, \( \beta = [2, -1, 3] \).
- Generate the response variable \( Y \) using the model \( Y = X\beta + \varepsilon \), where \( \varepsilon \) is drawn from a normal distribution \( N(0, 1) \).

```{r}
set.seed(1)
# Step 1: Generate Data
n <- 100
p <- 3

# Generate X (n x p matrix) with entries from N(0, 1)
X <- matrix(rnorm(n * p), nrow = n, ncol = p)

# True coefficient vector beta
beta <- c(2, -1, 3)

# Generate the error term (from N(0, 1))
epsilon <- rnorm(n)

# Generate Y = X * beta + epsilon
Y <- X %*% beta + epsilon


```

### Step 2: Perform OLS on \( X \)

Use R's `lm()` function to fit an OLS regression model of \( Y \) on \( X \) (without an intercept term). This will give us the coefficient estimates \( \hat{\beta} \), the fitted values \( \hat{Y} \), and the residuals \( \hat{\varepsilon} \).

```{r}
# Step 2: Perform OLS on X
fit_X <- lm(Y ~ X - 1)  
hat_beta <- coef(fit_X)
hat_Y <- fitted(fit_X)
hat_epsilon <- resid(fit_X)
```

### Step 3: Define a Transformation Matrix \( \Gamma \)

In this step, we define a random \( p \times p \) matrix \( \Gamma \), ensuring that it is invertible (non-degenerate). This matrix \( \Gamma \) will transform our predictor matrix \( X \). We then compute the transformed matrix \( \tilde{X} = X \Gamma \).

To ensure that \( \Gamma \) is invertible, we will check its determinant. If the determinant is zero, we will regenerate \( \Gamma \), as a matrix with a zero determinant is not invertible.

```{r}
# Step 3: Define a random transformation matrix Gamma (p x p)
#For this first attempt, set Gamma whose values are from the standard normal
Gamma <- matrix(rnorm(p * p), nrow = p, ncol = p)

# Check if Gamma is invertible (non-degenerate)
if (det(Gamma) == 0) {
  stop("Gamma is not invertible, regenerate Gamma.")
}

# Compute the transformed matrix X_tilde = X %*% Gamma
X_tilde <- X %*% Gamma
```
### Step 4: Perform OLS on \( \tilde{X} \)

Now, we will perform OLS regression on \( Y \) using the transformed predictor matrix \( \tilde{X} = X \Gamma \). Similar to the previous OLS fit, we will use the `lm()` function without an intercept term. This will give us the coefficient estimates \( \tilde{\beta} \), fitted values \( \tilde{Y} \), and residuals \( \tilde{\varepsilon} \).

```{r}
# Step 4: Perform OLS on X_tilde
fit_X_tilde <- lm(Y ~ X_tilde - 1)  # -1 to exclude intercept
tilde_beta <- coef(fit_X_tilde)
tilde_Y <- fitted(fit_X_tilde)
tilde_epsilon <- resid(fit_X_tilde)
```

### Step 5: Verify the relationships
```{r}
# 1. Check if hat_beta = Gamma %*% tilde_beta
hat_beta_computed <- as.numeric(Gamma %*% tilde_beta)
hat_beta <- as.numeric(hat_beta)

cat("Is hat_beta equal to Gamma %*% tilde_beta? ", all.equal(hat_beta, hat_beta_computed,tolerance = 1e-8),  "\n")

# 2. Check if hat_Y = tilde_Y
cat("Is hat_Y equal to tilde_Y? ", all.equal(hat_Y, tilde_Y), "\n")

# 3. Check if hat_epsilon = tilde_epsilon
cat("Is hat_epsilon equal to tilde_epsilon? ", all.equal(hat_epsilon, tilde_epsilon), "\n")
```


**Exercise 11**. Assume that \( X^TX \) is non-degenerate and \( \Gamma \) is a \( p \times p \) non-degenerate matrix. Define \( \tilde{X} = X\Gamma \). From the OLS fit of \( Y \) on \( X \), we obtain the coefficient \( \hat{\beta} \), the fitted value \( \hat{Y} \), and the residual \( \hat{\varepsilon} \); from the OLS fit of \( Y \) on \( \tilde{X} \), we obtain the coefficient \( \tilde{\beta} \), the fitted value \( \tilde{Y} \), and the residual \( \tilde{\varepsilon} \).

Prove that
\[
    \hat{\beta} = \Gamma\tilde{\beta}, \quad \hat{Y} = \tilde{Y}, \quad \hat{\varepsilon} = \tilde{\varepsilon}.
\]

## Full Sample and Subsample OLS Coefficients in R

Partition a dataset into \( K \) subsamples, fit OLS regression models on each subsample and the full sample, and validate the relationship:
\[
\hat{\beta} = \sum_{k = 1}^K W_{(k)}\hat{\beta}_{(k)},
\]
where \( W_{(k)} = (X^TX)^{-1}X_{(k)}^TX_{(k)} \).

---

### Step 1: Simulate Data

We begin by generating a full dataset:

- \( n = 100 \) observations, \( p = 3 \) predictors.
- Simulate a covariate matrix \( X \in \mathbb{R}^{n \times p} \) with entries from \( N(0, 1) \).
- Use a true coefficient vector \( \beta = [2, -1, 3] \).
- Outcome variable \( Y = X\beta + \varepsilon \), where \( \varepsilon \sim N(0, 1) \).

```{r}
set.seed(1)
# Step 1: Simulate data
n <- 100
p <- 3
K <- 4  # Number of subsamples

# Covariate matrix X (n x p) with entries from N(0, 1)
X <- matrix(rnorm(n * p), nrow = n, ncol = p)

# True coefficient vector beta
beta_true <- c(2, -1, 3)

# Generate the error term
epsilon <- rnorm(n)

# Response vector Y = X * beta + epsilon
Y <- X %*% beta_true + epsilon
```
### Step 2: Partition the Data into \( K \) Subsamples

In this step, we will partition the data into \( K = 4 \) subsamples. Each subsample \( (X_{(k)}, Y_{(k)}) \) will consist of approximately \( n_k = n / K \) observations. We will randomly assign the indices to create these subsamples.

```{r}
# Step 2: Partition the data into K subsamples
n_k <- floor(n / K)  # Number of observations per subsample
indices <- sample(rep(1:K, each = n_k))  # Randomly assign indices
```
### Step 3: Fit OLS for Full Sample and Subsamples

In this step, we will fit OLS regression models for both the full sample and each of the \( K \) subsamples. We will obtain the following:
- The OLS coefficient \( \hat{\beta} \) from the full sample.
- The OLS coefficients \( \hat{\beta}_{(k)} \) from each subsample.

We will use the `lm()` function in R to perform these regressions.

```{r}
# Step 3: Fit OLS for full sample
fit_full <- lm(Y ~ X - 1)  
beta_full <- coef(fit_full)  

# Fit OLS for each subsample
beta_subsamples <- lapply(1:K, function(k) {
  X_k <- X[which(indices == k), ]
  Y_k <- Y[which(indices == k)]
  coef_k <- coef(lm(Y_k ~ X_k - 1))
})

# Print the results
cat("Full sample OLS coefficients (beta_full):\n",beta_full, "\n")
for (k in 1:K) {
  cat(paste("Subsample", k, "OLS coefficients (beta_{(k)}):\n"), 
      beta_subsamples[[k]], "\n")
}
```

### Step 4: Compute the Weight Matrices

In this step, we will compute the weight matrices \( W_{(k)} \) for each subsample. The weight matrix is defined as:
\[
W_{(k)} = (X^TX)^{-1} X_{(k)}^TX_{(k)}.
\]

We will first compute \( (X^TX)^{-1} \) and then use it to calculate the weights for each subsample.

```{r}
# Step 4: Compute the weight matrices
XTX_inv <- solve(t(X) %*% X)  # Calculate (X^TX)^-1

# Compute the weight matrices for each subsample
W_matrices <- lapply(1:K, function(k) {
  X_k <- X[which(indices == k), ]
  t(X_k) %*% X_k %*% XTX_inv  # Compute W_{(k)}
  })

# Print the weight matrices
for (k in 1:K) {
  cat(paste("Weight matrix for subsample", k, "W_{(k)}:\n"), 
      W_matrices[[k]], "\n\n")
}

dim(W_matrices[[1]])

```
### Step 5: Verify the Weighted Sum Relationship

In this step, we will verify the relationship:
\[
\hat{\beta} = \sum_{k=1}^K W_{(k)}\hat{\beta}_{(k)},
\]
by computing the weighted sum of the subsample coefficients \( \hat{\beta}_{(k)} \) using the corresponding weight matrices \( W_{(k)} \) and comparing it with \( \hat{\beta} \) from the full sample.

```{r}
# Step 5: Verify the relationship
beta_weighted_sum <- Reduce("+", lapply(1:K, function(k) {
  W_matrices[[k]] %*% beta_subsamples[[k]]  # Compute W_{(k)} * beta_{(k)}
  }))
print(beta_weighted_sum)
print(beta_full)

```
**Exercise 12**. Partition the full sample into \( K \) subsamples:

\begin{eqnarray*}
    X = \begin{pmatrix}
        X_{(1)} \\
        \vdots \\
        X_{(K)} 
    \end{pmatrix}, \quad Y = \begin{pmatrix}
        Y_{(1)} \\
        \vdots \\
        Y_{(K)} 
    \end{pmatrix},
\end{eqnarray*}

where the \( k \)th sample consists of \( (X_{(k)}, Y_{(k)}) \) with \( X_{(k)} \in \mathbb{R}^{n_k \times p} \) and \( Y_{(k)} \in \mathbb{R}^{n_k} \) being the covariate matrix and outcome vector. Note that \( n = \sum_{k = 1}^K n_k \).

Let \( \hat{\beta} \) be the OLS coefficient based on the full sample, and \( \hat{\beta}_{(k)} \) be the OLS coefficient based on the \( k \)th sample. Show that
\[
\hat{\beta} = \sum_{k = 1}^K W_{(k)}\hat{\beta}_{(k)},
\]
where the weight matrix equals 
\[
W_{(k)} = (X^TX)^{-1}X_{(k)}^TX_{(k)}.
\]
